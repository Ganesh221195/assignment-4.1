Tv.java

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

public class Tv
 {
    @SuppressWarnings("deprecation")
    public static void main(String[] args) throws Exception 
{
    Configuration conf = new Configuration();
Job job = new Job(conf, "DemoTask1");
job.setJarByClass(Tv.class);
job.setMapOutputKeyClass(IntWritable.class);
job.setMapOutputValueClass(Text.class);
job.setOutputKeyClass(IntWritable.class);
job.setOutputValueClass(Text.class);
job.setMapperClass(Tvmapper.class);
job.setReducerClass(Tvreducer.class);
    job.setInputFormatClass(TextInputFormat.class);
job.setOutputFormatClass(TextOutputFormat.class);
FileInputFormat.addInputPath(job, new Path(args[0])); 
FileOutputFormat.setOutputPath(job,new Path(args[1]));
        /*Path out=new Path(args[1]);out.getFileSystem(conf).delete(out);    */
    job.waitForCompletion(true);
}
}

=======================================================================

Tvmapper.java


import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.*; 
public class Tvmapper extends Mapper<LongWritable, Text, IntWritable, Text>
 {
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException 
{
int label=0;
        String[] lineArray = value.toString().split("|");
for(String ts : lineArray)
{
if(ts.equals("NA"))
{
label=1;}
}
IntWritable ki = new IntWritable(label);
Text v = value;
context.write(ki,v);
    }
}

=================================================================================

Tvreducer.java

import java.io.IOException;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
public class Tvreducer extends Reducer<IntWritable, Text, IntWritable, Text>
{
    public void reduce(IntWritable key, Iterable<Text> values,Context context) throws IOException, InterruptedException
    {
    if(key.equals("0")){
        
context.write(key, (Text) values);
}
    }
}